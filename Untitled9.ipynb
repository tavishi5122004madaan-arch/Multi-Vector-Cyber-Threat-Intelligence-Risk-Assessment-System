{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "LUTqnKX9_9-V",
        "outputId": "680777dc-0ea6-4704-9d44-004a4ee5a332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ee3c6b8a80bc5105ff.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ee3c6b8a80bc5105ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# ==========================================\n",
        "# CyberAI Suite: URL, Email & Script Malware Detector (Gradio App)\n",
        "# Features:\n",
        "# 1. URL, Email & Script Detection\n",
        "# 2. Synthetic Data Generation\n",
        "# 3. Random Forest Models\n",
        "# 4. Real-time Adaptive Learning\n",
        "# 5. Combined Cyber Threat Score\n",
        "# 6. Explainable AI (SHAP)\n",
        "# 7. 51 Features across all modules\n",
        "# ==========================================\n",
        "\n",
        "import random\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import shap\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ FEATURE EXTRACTION\n",
        "# =========================\n",
        "\n",
        "SUSPICIOUS_WORDS = [\"login\", \"secure\", \"verify\", \"update\", \"account\", \"bank\"]\n",
        "BRAND_NAMES = [\"google\", \"paypal\", \"amazon\", \"facebook\", \"apple\"]\n",
        "SUSP_WORDS_EMAIL = [\"verify\",\"urgent\",\"login\",\"payment\",\"account\",\"update\"]\n",
        "SUSPICIOUS_FUNCTIONS = [\"eval\",\"unescape\",\"escape\",\"document.write\",\"window.location\",\"setTimeout\",\"setInterval\",\"alert\"]\n",
        "\n",
        "def url_entropy(s):\n",
        "    probs = [float(s.count(c))/len(s) for c in set(s)]\n",
        "    return -sum([p*math.log2(p) for p in probs])\n",
        "\n",
        "def extract_url_features(url):\n",
        "    features = {}\n",
        "    features['length'] = len(url)\n",
        "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
        "    features['num_special'] = sum(c in string.punctuation for c in url)\n",
        "    features['num_subdomains'] = url.count('.') - 1\n",
        "    features['has_https'] = int(url.startswith(\"https\"))\n",
        "    features['num_hyphens'] = url.count('-')\n",
        "    features['num_underscores'] = url.count('_')\n",
        "    features['num_query_params'] = url.count('?')\n",
        "    features['num_fragments'] = url.count('#')\n",
        "    features['numbers_in_domain'] = sum(c.isdigit() for c in url.split(\"//\")[-1].split(\"/\")[0])\n",
        "    features['entropy'] = url_entropy(url)\n",
        "    features['has_suspicious_word'] = int(any(word in url.lower() for word in SUSPICIOUS_WORDS))\n",
        "    features['has_brand_name'] = int(any(brand in url.lower() for brand in BRAND_NAMES))\n",
        "    features['repeated_chars'] = int(any(url[i]==url[i+1]==url[i+2] for i in range(len(url)-2)))\n",
        "    domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
        "    features['domain_length'] = len(domain)\n",
        "    path = url.split(domain)[-1]\n",
        "    features['path_length'] = len(path)\n",
        "    features['dots_in_path'] = path.count('.')\n",
        "    features['letters_to_numbers_ratio'] = sum(c.isalpha() for c in url)/max(1,sum(c.isdigit() for c in url))\n",
        "    features['ends_with_common_tld'] = int(url.endswith(('.com','.net','.org','.gov')))\n",
        "    features['starts_with_ip'] = int(re.match(r'http[s]?://\\d+\\.\\d+\\.\\d+\\.\\d+', url) is not None)\n",
        "    return features\n",
        "\n",
        "def extract_email_features(subject):\n",
        "    features = {}\n",
        "    features['length'] = len(subject)\n",
        "    words = subject.split()\n",
        "    features['num_suspicious_words'] = sum(word.lower() in SUSP_WORDS_EMAIL for word in words)\n",
        "    features['num_exclamations'] = subject.count('!')\n",
        "    features['num_questions'] = subject.count('?')\n",
        "    features['num_uppercase_words'] = sum(1 for w in words if w.isupper())\n",
        "    features['num_lowercase_words'] = sum(1 for w in words if w.islower())\n",
        "    features['uppercase_to_lowercase_ratio'] = features['num_uppercase_words']/max(1,features['num_lowercase_words'])\n",
        "    features['has_link'] = int(any(x in subject.lower() for x in [\"http\",\"https\"]))\n",
        "    features['has_attachment'] = int(any(x in subject.lower() for x in [\".pdf\",\".exe\",\".zip\"]))\n",
        "    features['num_digits'] = sum(c.isdigit() for c in subject)\n",
        "    features['num_special_chars'] = sum(c in string.punctuation for c in subject)\n",
        "    features['starts_with_greeting'] = int(any(subject.lower().startswith(g) for g in [\"dear\",\"hi\",\"hello\"]))\n",
        "    features['ends_with_signature'] = int(any(subject.lower().endswith(s) for s in [\"regards\",\"thanks\",\"thank you\"]))\n",
        "    features['num_spaces'] = subject.count(' ')\n",
        "    features['num_words'] = len(words)\n",
        "    return features\n",
        "\n",
        "def extract_script_features(script):\n",
        "    features = {}\n",
        "    features['length'] = len(script)\n",
        "    features['num_lines'] = script.count('\\n') + 1\n",
        "    features['has_eval'] = int('eval' in script)\n",
        "    features['has_iframe'] = int('iframe' in script)\n",
        "    features['has_window_location'] = int('window.location' in script)\n",
        "    features['num_document_write'] = script.count('document.write')\n",
        "    features['num_settimeout'] = script.count('setTimeout') + script.count('setInterval')\n",
        "    features['num_suspicious_functions'] = sum(script.count(f) for f in SUSPICIOUS_FUNCTIONS)\n",
        "    features['num_concatenations'] = script.count('+')\n",
        "    features['num_comments'] = script.count('//') + script.count('/*')\n",
        "    letters = sum(c.isalpha() for c in script)\n",
        "    numbers = sum(c.isdigit() for c in script)\n",
        "    features['letters_to_numbers_ratio'] = letters/max(1,numbers)\n",
        "    features['special_char_ratio'] = sum(c in string.punctuation for c in script)/max(1,len(script))\n",
        "    features['num_var_declarations'] = script.count('var') + script.count('let') + script.count('const')\n",
        "    features['num_functions'] = script.count('function')\n",
        "    features['num_alerts'] = script.count('alert')\n",
        "    features['has_base64'] = int('base64' in script.lower())\n",
        "    return features\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ SYNTHETIC DATA GENERATION & MODELS\n",
        "# =========================\n",
        "\n",
        "def generate_url_dataset(n=500):\n",
        "    data, labels = [], []\n",
        "    for _ in range(n):\n",
        "        malicious = random.choice([0,1])\n",
        "        domain = ''.join(random.choices(string.ascii_lowercase, k=6))\n",
        "        extra = random.choice([\"login\",\"secure\",\"verify\",\"update\"]) if malicious else \"\"\n",
        "        url = f\"http://{domain}{extra}.com\"\n",
        "        data.append(extract_url_features(url))\n",
        "        labels.append(malicious)\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "def generate_email_dataset(n=500):\n",
        "    data, labels = [], []\n",
        "    for _ in range(n):\n",
        "        malicious = random.choice([0,1])\n",
        "        subject = random.choice([\"Verify Account\",\"Urgent Action Required\",\"Login Now\"]) if malicious else random.choice([\"Meeting Reminder\",\"Project Update\",\"Greetings\"])\n",
        "        data.append(extract_email_features(subject))\n",
        "        labels.append(malicious)\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "def generate_script_dataset(n=500):\n",
        "    data, labels = [], []\n",
        "    for _ in range(n):\n",
        "        malicious = random.choice([0,1])\n",
        "        script = random.choice([\"eval('bad')\",\"document.write('<iframe>')\",\"window.location='evil.com';\"]) if malicious else \"console.log('hello');\"\n",
        "        data.append(extract_script_features(script))\n",
        "        labels.append(malicious)\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# Train models\n",
        "url_df = generate_url_dataset(500)\n",
        "X_url, y_url = url_df.drop('label', axis=1), url_df['label']\n",
        "url_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "url_model.fit(X_url, y_url)\n",
        "\n",
        "email_df = generate_email_dataset(500)\n",
        "X_email, y_email = email_df.drop('label', axis=1), email_df['label']\n",
        "email_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "email_model.fit(X_email, y_email)\n",
        "\n",
        "script_df = generate_script_dataset(500)\n",
        "X_script, y_script = script_df.drop('label', axis=1), script_df['label']\n",
        "script_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "script_model.fit(X_script, y_script)\n",
        "\n",
        "# SHAP explainers\n",
        "url_explainer = shap.TreeExplainer(url_model)\n",
        "email_explainer = shap.TreeExplainer(email_model)\n",
        "script_explainer = shap.TreeExplainer(script_model)\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ GRADIO INTERFACE FUNCTIONS\n",
        "# =========================\n",
        "\n",
        "def cyber_ai(url_input, email_input, script_input):\n",
        "    # URL\n",
        "    url_feat = extract_url_features(url_input)\n",
        "    url_df_input = pd.DataFrame([url_feat])\n",
        "    url_prob = url_model.predict_proba(url_df_input)[0][1]\n",
        "\n",
        "    # Email\n",
        "    email_feat = extract_email_features(email_input)\n",
        "    email_df_input = pd.DataFrame([email_feat])\n",
        "    email_prob = email_model.predict_proba(email_df_input)[0][1]\n",
        "\n",
        "    # Script\n",
        "    script_feat = extract_script_features(script_input)\n",
        "    script_df_input = pd.DataFrame([script_feat])\n",
        "    script_prob = script_model.predict_proba(script_df_input)[0][1]\n",
        "\n",
        "    # Combined score\n",
        "    threat_score = np.mean([url_prob,email_prob,script_prob])\n",
        "    threat_level = \"Low Threat ‚úÖ\" if threat_score<0.4 else \"Medium Threat ‚ö†Ô∏è\" if threat_score<0.7 else \"High Threat üö®\"\n",
        "\n",
        "    return f\"URL Malicious Probability: {url_prob*100:.2f}%\\n\" + \\\n",
        "           f\"Email Malicious Probability: {email_prob*100:.2f}%\\n\" + \\\n",
        "           f\"Script Malicious Probability: {script_prob*100:.2f}%\\n\" + \\\n",
        "           f\"Overall Cyber Threat Score: {threat_score*100:.2f}%\\n\" + \\\n",
        "           f\"Threat Level: {threat_level}\"\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ GRADIO APP\n",
        "# =========================\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=cyber_ai,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter URL\"),\n",
        "        gr.Textbox(label=\"Enter Email Subject\"),\n",
        "        gr.Textbox(label=\"Enter Script (JS/HTML)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Cyber Threat Analysis\"),\n",
        "    title=\"üöÄ CyberAI Suite\",\n",
        "    description=\"Real-time detection of malicious URLs, emails, and scripts with combined threat scoring.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install reportlab\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "import zlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse\n",
        "import unicodedata\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import A4\n",
        "\n",
        "# ===============================\n",
        "# GLOBAL CONSTANTS\n",
        "# ===============================\n",
        "\n",
        "SUSPICIOUS_WORDS = [\"login\",\"verify\",\"bank\",\"secure\",\"update\",\n",
        "                    \"account\",\"password\",\"confirm\",\"urgent\"]\n",
        "\n",
        "SOCIAL_ENGINEERING_WORDS = [\"urgent\",\"immediately\",\"action required\",\n",
        "                            \"suspend\",\"limited time\",\"verify now\",\n",
        "                            \"click below\",\"security alert\"]\n",
        "\n",
        "SUSPICIOUS_TLDS = [\".tk\",\".ml\",\".ga\",\".cf\",\".xyz\",\".top\"]\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ADVANCED FEATURE FUNCTIONS\n",
        "# ===============================\n",
        "\n",
        "def shannon_entropy(text):\n",
        "    if not text:\n",
        "        return 0\n",
        "    prob = [text.count(c)/len(text) for c in set(text)]\n",
        "    return -sum(p * math.log2(p) for p in prob)\n",
        "\n",
        "def compression_ratio(text):\n",
        "    if not text:\n",
        "        return 0\n",
        "    compressed = zlib.compress(text.encode())\n",
        "    return len(compressed)/len(text.encode())\n",
        "\n",
        "def homoglyph_count(text):\n",
        "    count = 0\n",
        "    for c in text:\n",
        "        try:\n",
        "            if \"CYRILLIC\" in unicodedata.name(c):\n",
        "                count += 1\n",
        "        except:\n",
        "            continue\n",
        "    return count\n",
        "\n",
        "def bigram_anomaly_score(text):\n",
        "    if len(text) < 2:\n",
        "        return 0\n",
        "    bigrams = [text[i:i+2] for i in range(len(text)-1)]\n",
        "    freq = Counter(bigrams)\n",
        "    return np.std(list(freq.values()))\n",
        "\n",
        "def keyword_density(text, keywords):\n",
        "    words = text.lower().split()\n",
        "    if not words:\n",
        "        return 0\n",
        "    return sum(word in keywords for word in words)/len(words)\n",
        "\n",
        "def randomness_index(text):\n",
        "    if not text:\n",
        "        return 0\n",
        "    return shannon_entropy(text)/max(1,len(set(text)))\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# FEATURE EXTRACTION\n",
        "# ===============================\n",
        "\n",
        "def extract_url_features(url):\n",
        "    parsed = urlparse(url)\n",
        "    return {\n",
        "        'length': len(url),\n",
        "        'dots': url.count('.'),\n",
        "        'hyphens': url.count('-'),\n",
        "        'digits': sum(c.isdigit() for c in url),\n",
        "        'suspicious_words': sum(word in url.lower() for word in SUSPICIOUS_WORDS),\n",
        "        'tld_risk': int(any(url.endswith(tld) for tld in SUSPICIOUS_TLDS)),\n",
        "        'path_depth': url.count('/'),\n",
        "        'entropy': shannon_entropy(url),\n",
        "        'compression_ratio': compression_ratio(url),\n",
        "        'homoglyphs': homoglyph_count(url),\n",
        "        'bigram_anomaly': bigram_anomaly_score(url),\n",
        "        'randomness_index': randomness_index(url),\n",
        "        'keyword_density': keyword_density(url, SUSPICIOUS_WORDS)\n",
        "    }\n",
        "\n",
        "def extract_email_features(subject):\n",
        "    return {\n",
        "        'length': len(subject),\n",
        "        'uppercase_ratio': sum(c.isupper() for c in subject)/max(1,len(subject)),\n",
        "        'exclamations': subject.count('!'),\n",
        "        'suspicious_words': sum(word in subject.lower() for word in SUSPICIOUS_WORDS),\n",
        "        'entropy': shannon_entropy(subject),\n",
        "        'social_engineering_score': keyword_density(subject, SOCIAL_ENGINEERING_WORDS),\n",
        "        'compression_ratio': compression_ratio(subject),\n",
        "        'bigram_anomaly': bigram_anomaly_score(subject),\n",
        "        'randomness_index': randomness_index(subject),\n",
        "        'homoglyphs': homoglyph_count(subject)\n",
        "    }\n",
        "\n",
        "def extract_script_features(script):\n",
        "    return {\n",
        "        'length': len(script),\n",
        "        'eval_usage': script.count(\"eval\"),\n",
        "        'document_write': script.count(\"document.write\"),\n",
        "        'iframe': script.count(\"iframe\"),\n",
        "        'entropy': shannon_entropy(script),\n",
        "        'compression_ratio': compression_ratio(script),\n",
        "        'bigram_anomaly': bigram_anomaly_score(script),\n",
        "        'randomness_index': randomness_index(script),\n",
        "        'encoded_strings': script.count(\"atob\") + script.count(\"btoa\"),\n",
        "        'hex_encoding': script.count(\"\\\\x\"),\n",
        "        'charcode_usage': script.count(\"charCodeAt\"),\n",
        "        'dynamic_execution': script.count(\"Function(\")\n",
        "    }\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# SCORING\n",
        "# ===============================\n",
        "\n",
        "def calculate_score(features):\n",
        "    return sum(features.values())\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# VISUALIZATION\n",
        "# ===============================\n",
        "\n",
        "def create_visuals(url_f, email_f, script_f):\n",
        "\n",
        "    # Radar Graph\n",
        "    categories = list(url_f.keys())[:6]\n",
        "    values = list(url_f.values())[:6]\n",
        "    values += values[:1]\n",
        "    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    fig1 = plt.figure()\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    ax.plot(angles, values)\n",
        "    ax.fill(angles, values, alpha=0.25)\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(categories)\n",
        "    plt.title(\"URL Risk Radar\")\n",
        "\n",
        "    # Heatmap\n",
        "    combined = np.array([\n",
        "        list(url_f.values())[:10],\n",
        "        list(email_f.values())[:10],\n",
        "        list(script_f.values())[:10]\n",
        "    ])\n",
        "\n",
        "    fig2 = plt.figure()\n",
        "    plt.imshow(combined)\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Threat Heatmap\")\n",
        "\n",
        "    # 3D Plot\n",
        "    fig3 = plt.figure()\n",
        "    ax3 = fig3.add_subplot(111, projection='3d')\n",
        "    ax3.scatter(\n",
        "        calculate_score(url_f),\n",
        "        calculate_score(email_f),\n",
        "        calculate_score(script_f)\n",
        "    )\n",
        "    ax3.set_xlabel(\"URL Score\")\n",
        "    ax3.set_ylabel(\"Email Score\")\n",
        "    ax3.set_zlabel(\"Script Score\")\n",
        "    plt.title(\"3D Threat Landscape\")\n",
        "\n",
        "    return fig1, fig2, fig3\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# PDF REPORT\n",
        "# ===============================\n",
        "\n",
        "def generate_pdf(url_score, email_score, script_score, overall):\n",
        "    file_path = \"Cyber_Threat_Report.pdf\"\n",
        "    doc = SimpleDocTemplate(file_path, pagesize=A4)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    elements.append(Paragraph(\"PhD-Level Cyber Threat Intelligence Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "    elements.append(Paragraph(f\"URL Threat Score: {url_score}\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Email Threat Score: {email_score}\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Script Threat Score: {script_score}\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Overall Cyber Threat Score: {overall}\", styles['Heading2']))\n",
        "\n",
        "    doc.build(elements)\n",
        "    return file_path\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# MAIN ANALYSIS FUNCTION\n",
        "# ===============================\n",
        "\n",
        "def analyze(url, subject, script):\n",
        "\n",
        "    url_f = extract_url_features(url)\n",
        "    email_f = extract_email_features(subject)\n",
        "    script_f = extract_script_features(script)\n",
        "\n",
        "    url_score = calculate_score(url_f)\n",
        "    email_score = calculate_score(email_f)\n",
        "    script_score = calculate_score(script_f)\n",
        "\n",
        "    overall = url_score + email_score + script_score\n",
        "\n",
        "    fig1, fig2, fig3 = create_visuals(url_f, email_f, script_f)\n",
        "    pdf_file = generate_pdf(url_score, email_score, script_score, overall)\n",
        "\n",
        "    result_text = f\"\"\"\n",
        "    URL Score: {url_score}\n",
        "    Email Score: {email_score}\n",
        "    Script Score: {script_score}\n",
        "    OVERALL CYBER THREAT SCORE: {overall}\n",
        "    \"\"\"\n",
        "\n",
        "    return result_text, fig1, fig2, fig3, pdf_file\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# GRADIO APP\n",
        "# ===============================\n",
        "\n",
        "with gr.Blocks(title=\"PhD-Level Cyber Threat Intelligence System\") as app:\n",
        "\n",
        "    gr.Markdown(\"# üîê PhD-Level Cyber Threat Intelligence System\")\n",
        "\n",
        "    url_input = gr.Textbox(label=\"Enter URL\")\n",
        "    email_input = gr.Textbox(label=\"Enter Email Subject\")\n",
        "    script_input = gr.Textbox(label=\"Enter Script (JS/HTML)\")\n",
        "\n",
        "    analyze_btn = gr.Button(\"Analyze Threat\")\n",
        "\n",
        "    output_text = gr.Textbox(label=\"Threat Scores\")\n",
        "    radar_plot = gr.Plot(label=\"Radar Graph\")\n",
        "    heatmap_plot = gr.Plot(label=\"Heatmap\")\n",
        "    plot3d = gr.Plot(label=\"3D Threat Landscape\")\n",
        "    pdf_output = gr.File(label=\"Download PDF Report\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze,\n",
        "        inputs=[url_input, email_input, script_input],\n",
        "        outputs=[output_text, radar_plot, heatmap_plot, plot3d, pdf_output]\n",
        "    )\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "id": "ohpt0Mgyajsl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "outputId": "bd2c3ef7-7e8a-441c-96aa-383522204324"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.10-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Downloading reportlab-4.4.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.10\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://55835b498c538d00c5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://55835b498c538d00c5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import math\n",
        "import zlib\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse\n",
        "import unicodedata\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import A4\n",
        "\n",
        "# ===============================\n",
        "# GLOBAL CONSTANTS\n",
        "# ===============================\n",
        "\n",
        "SUSPICIOUS_WORDS = [\"login\",\"verify\",\"bank\",\"secure\",\"update\",\n",
        "                    \"account\",\"password\",\"confirm\",\"urgent\"]\n",
        "\n",
        "SOCIAL_ENGINEERING_WORDS = [\"urgent\",\"immediately\",\"action required\",\n",
        "                            \"suspend\",\"limited time\",\"verify now\",\n",
        "                            \"click below\",\"security alert\"]\n",
        "\n",
        "SUSPICIOUS_TLDS = [\".tk\",\".ml\",\".ga\",\".cf\",\".xyz\",\".top\"]\n",
        "\n",
        "# ===============================\n",
        "# ADVANCED FUNCTIONS\n",
        "# ===============================\n",
        "\n",
        "def shannon_entropy(text):\n",
        "    if not text:\n",
        "        return 0\n",
        "    prob = [text.count(c)/len(text) for c in set(text)]\n",
        "    return -sum(p * math.log2(p) for p in prob)\n",
        "\n",
        "def compression_ratio(text):\n",
        "    if not text:\n",
        "        return 0\n",
        "    compressed = zlib.compress(text.encode())\n",
        "    return len(compressed)/len(text.encode())\n",
        "\n",
        "def homoglyph_count(text):\n",
        "    count = 0\n",
        "    for c in text:\n",
        "        try:\n",
        "            if \"CYRILLIC\" in unicodedata.name(c):\n",
        "                count += 1\n",
        "        except:\n",
        "            continue\n",
        "    return count\n",
        "\n",
        "def bigram_anomaly_score(text):\n",
        "    if len(text) < 2:\n",
        "        return 0\n",
        "    bigrams = [text[i:i+2] for i in range(len(text)-1)]\n",
        "    freq = Counter(bigrams)\n",
        "    return np.std(list(freq.values()))\n",
        "\n",
        "def keyword_density(text, keywords):\n",
        "    words = text.lower().split()\n",
        "    if not words:\n",
        "        return 0\n",
        "    return sum(word in keywords for word in words)/len(words)\n",
        "\n",
        "# ===============================\n",
        "# FEATURE EXTRACTION\n",
        "# ===============================\n",
        "\n",
        "def extract_url_features(url):\n",
        "    return {\n",
        "        'Length': len(url),\n",
        "        'Dots': url.count('.'),\n",
        "        'Hyphens': url.count('-'),\n",
        "        'Digits': sum(c.isdigit() for c in url),\n",
        "        'Suspicious Words': sum(word in url.lower() for word in SUSPICIOUS_WORDS),\n",
        "        'TLD Risk': int(any(url.endswith(tld) for tld in SUSPICIOUS_TLDS)),\n",
        "        'Path Depth': url.count('/'),\n",
        "        'Entropy': shannon_entropy(url),\n",
        "        'Compression': compression_ratio(url),\n",
        "        'Homoglyphs': homoglyph_count(url),\n",
        "        'Bigram Anomaly': bigram_anomaly_score(url),\n",
        "        'Keyword Density': keyword_density(url, SUSPICIOUS_WORDS)\n",
        "    }\n",
        "\n",
        "def extract_email_features(subject):\n",
        "    return {\n",
        "        'Length': len(subject),\n",
        "        'Uppercase Ratio': sum(c.isupper() for c in subject)/max(1,len(subject)),\n",
        "        'Exclamations': subject.count('!'),\n",
        "        'Suspicious Words': sum(word in subject.lower() for word in SUSPICIOUS_WORDS),\n",
        "        'Entropy': shannon_entropy(subject),\n",
        "        'Social Engineering': keyword_density(subject, SOCIAL_ENGINEERING_WORDS),\n",
        "        'Compression': compression_ratio(subject),\n",
        "        'Bigram Anomaly': bigram_anomaly_score(subject),\n",
        "        'Homoglyphs': homoglyph_count(subject)\n",
        "    }\n",
        "\n",
        "def extract_script_features(script):\n",
        "    return {\n",
        "        'Length': len(script),\n",
        "        'Eval Usage': script.count(\"eval\"),\n",
        "        'Iframe': script.count(\"iframe\"),\n",
        "        'Entropy': shannon_entropy(script),\n",
        "        'Compression': compression_ratio(script),\n",
        "        'Bigram Anomaly': bigram_anomaly_score(script),\n",
        "        'Encoded Strings': script.count(\"atob\") + script.count(\"btoa\"),\n",
        "        'Hex Encoding': script.count(\"\\\\x\"),\n",
        "        'Dynamic Execution': script.count(\"Function(\")\n",
        "    }\n",
        "\n",
        "# ===============================\n",
        "# NORMALIZATION TO PERCENTAGE\n",
        "# ===============================\n",
        "\n",
        "def normalize_score(score, max_possible=150):\n",
        "    return min(100, (score / max_possible) * 100)\n",
        "\n",
        "def calculate_percentage(features):\n",
        "    raw_score = sum(features.values())\n",
        "    return normalize_score(raw_score)\n",
        "\n",
        "# ===============================\n",
        "# VISUALIZATION (ALL INTERACTIVE) # Ensure plotly is imported as px, go\n",
        "# ===============================\n",
        "\n",
        "def create_visuals(url_f, email_f, script_f, url_p, email_p, script_p):\n",
        "\n",
        "    # Radar\n",
        "    radar = go.Figure()\n",
        "    radar.add_trace(go.Scatterpolar(\n",
        "        r=list(url_f.values()),\n",
        "        theta=list(url_f.keys()),\n",
        "        fill='toself',\n",
        "        name='URL'\n",
        "    ))\n",
        "    radar.update_layout(title=\"URL Risk Radar\", polar=dict(radialaxis=dict(visible=True)))\n",
        "    # Removed radar.show()\n",
        "\n",
        "    # Heatmap\n",
        "    # Get the number of features for each type\n",
        "    num_url_features = len(url_f)\n",
        "    num_email_features = len(email_f)\n",
        "    num_script_features = len(script_f)\n",
        "\n",
        "    # Determine the maximum number of features\n",
        "    max_num_features = max(num_url_features, num_email_features, num_script_features)\n",
        "\n",
        "    # Pad feature lists to match the maximum number of features\n",
        "    url_values_padded = list(url_f.values()) + [0] * (max_num_features - num_url_features)\n",
        "    email_values_padded = list(email_f.values()) + [0] * (max_num_features - num_email_features)\n",
        "    script_values_padded = list(script_f.values()) + [0] * (max_num_features - num_script_features)\n",
        "\n",
        "    heat_data = [\n",
        "        url_values_padded,\n",
        "        email_values_padded,\n",
        "        script_values_padded\n",
        "    ]\n",
        "\n",
        "    # For x-axis labels, use the keys from the dictionary that has the most features.\n",
        "    # Assuming url_f is the longest as per feature extraction logic.\n",
        "    x_labels = list(url_f.keys()) + [''] * (max_num_features - num_url_features)\n",
        "\n",
        "    heatmap = px.imshow(heat_data,\n",
        "                        labels=dict(x=\"Features\", y=\"Module\", color=\"Risk\"),\n",
        "                        x=x_labels, # Use the padded x-axis labels\n",
        "                        y=[\"URL\",\"Email\",\"Script\"],\n",
        "                        title=\"Threat Heatmap\")\n",
        "    # Removed heatmap.show()\n",
        "\n",
        "    # Bar Graph (Percentage)\n",
        "    bar = go.Figure([go.Bar(\n",
        "        x=[\"URL\",\"Email\",\"Script\"],\n",
        "        y=[url_p, email_p, script_p]\n",
        "    )])\n",
        "    bar.update_layout(title=\"Threat Percentage Comparison\",\n",
        "                      yaxis_title=\"Risk %\")\n",
        "    # Removed bar.show()\n",
        "\n",
        "    # Interactive 3D\n",
        "    fig3d = go.Figure(data=[go.Scatter3d(\n",
        "        x=[url_p],\n",
        "        y=[email_p],\n",
        "        z=[script_p],\n",
        "        mode='markers',\n",
        "        marker=dict(size=10)\n",
        "    )])\n",
        "    fig3d.update_layout(\n",
        "        title=\"Interactive 3D Threat Landscape\",\n",
        "        scene=dict(\n",
        "            xaxis_title='URL %',\n",
        "            yaxis_title='Email %',\n",
        "            zaxis_title='Script %'\n",
        "        )\n",
        "    )\n",
        "    # Removed fig3d.show()\n",
        "\n",
        "    return radar, heatmap, bar, fig3d\n",
        "\n",
        "# ===============================\n",
        "# PDF REPORT\n",
        "# ===============================\n",
        "\n",
        "def generate_pdf(url_p, email_p, script_p, overall):\n",
        "    file_path = \"Cyber_Threat_Report.pdf\"\n",
        "    doc = SimpleDocTemplate(file_path, pagesize=A4)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    elements.append(Paragraph(\"PhD-Level Cyber Threat Intelligence Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "    elements.append(Paragraph(f\"URL Risk: {url_p:.2f}%\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Email Risk: {email_p:.2f}%\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Script Risk: {script_p:.2f}%\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Overall Threat Level: {overall:.2f}%\", styles['Heading2']))\n",
        "\n",
        "    doc.build(elements)\n",
        "    return file_path\n",
        "\n",
        "# ===============================\n",
        "# MAIN ANALYSIS # This is the function called by Gradio\n",
        "# ===============================\n",
        "\n",
        "def analyze(url, subject, script):\n",
        "\n",
        "    url_f = extract_url_features(url)\n",
        "    email_f = extract_email_features(subject)\n",
        "    script_f = extract_script_features(script)\n",
        "\n",
        "    url_p = calculate_percentage(url_f)\n",
        "    email_p = calculate_percentage(email_f)\n",
        "    script_p = calculate_percentage(script_f)\n",
        "\n",
        "    overall = (url_p + email_p + script_p) / 3\n",
        "\n",
        "    radar, heatmap, bar, fig3d = create_visuals(\n",
        "        url_f, email_f, script_f, url_p, email_p, script_p\n",
        "    )\n",
        "\n",
        "    pdf_file = generate_pdf(url_p, email_p, script_p, overall)\n",
        "\n",
        "    result = f\"\"\"\n",
        "    URL Risk: {url_p:.2f}%\n",
        "    Email Risk: {email_p:.2f}%\n",
        "    Script Risk: {script_p:.2f}%\n",
        "    OVERALL CYBER THREAT LEVEL: {overall:.2f}%\n",
        "    \"\"\"\n",
        "\n",
        "    return result, radar, heatmap, bar, fig3d, pdf_file\n",
        "\n",
        "# ===============================\n",
        "# GRADIO APP # Ensure this part remains for the Gradio interface\n",
        "# ===============================\n",
        "\n",
        "with gr.Blocks(title=\"PhD-Level Cyber Threat Intelligence System\") as app:\n",
        "\n",
        "    gr.Markdown(\"# üîê Advanced Cyber Threat Intelligence Dashboard\")\n",
        "\n",
        "    url_input = gr.Textbox(label=\"Enter URL\")\n",
        "    email_input = gr.Textbox(label=\"Enter Email Subject\")\n",
        "    script_input = gr.Textbox(label=\"Enter Script (JS/HTML)\")\n",
        "\n",
        "    analyze_btn = gr.Button(\"Analyze Threat\")\n",
        "\n",
        "    output_text = gr.Textbox(label=\"Threat Summary\")\n",
        "    radar_plot = gr.Plot(label=\"Radar\")\n",
        "    heatmap_plot = gr.Plot(label=\"Heatmap\")\n",
        "    bar_plot = gr.Plot(label=\"Bar Graph\")\n",
        "    plot3d = gr.Plot(label=\"Interactive 3D\")\n",
        "    pdf_output = gr.File(label=\"Download PDF Report\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze,\n",
        "        inputs=[url_input, email_input, script_input],\n",
        "        outputs=[output_text, radar_plot, heatmap_plot, bar_plot, plot3d, pdf_output]\n",
        "    )\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "9aYha1NV9YSw",
        "outputId": "fe73a59e-85f6-4135-ecba-9438a5ef17b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a5fbbd562773e29162.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a5fbbd562773e29162.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}